
---
title: "Wine Evaluation"
author: "Shaya Engelman"
date: "2024-12-18"
output:
  html_document
---

# Introduction

A data set containing information on approximately 12,000 commercially available wines and their variables mostly related to  chemical properties is analyzed for impact on sales and used to predict on sales to give accurate forecasts for manufacturing. The response variable is the number of sample cases of wine that were purchased by wine distribution companies after sampling a wine. These cases would be used to provide tasting samples to restaurants and wine stores around the United States. The more sample cases purchased, the more likely is a wine to be sold at a high end restaurant.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message = FALSE)


# include = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
# echo = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
# message = FALSE prevents messages that are generated by code from appearing in the finished file.
# warning = FALSE prevents warnings that are generated by code from appearing in the finished.
# fig.cap = "..." adds a caption to graphical results.
```


## Required Libraries

```{r library, warning=FALSE, message=FALSE, class.source = "fold-show", echo = TRUE}
library(tidyverse)
library(janitor)
library(knitr)
library(kableExtra)
library(latex2exp)
library(psych)
library(scales)
library(stringr)
library(ggcorrplot)
library(ggmice)
library(caret)
library(mice)
library(bestNormalize)
library(e1071)
library(diptest)
library(MASS)
library(performance)
```

# Data Exploration

To-Do List:
1. Check for typo's - No Typo's, all data is int
2. Check for missing
3. Show distributions
4. Determine Categorical/Continuous


```{r Load Data}

url <- "https://raw.githubusercontent.com/d-ev-craig/DATA621_Group/main/HW5/data/wine-training-data.csv"
eval_url <- "https://raw.githubusercontent.com/d-ev-craig/DATA621_Group/main/HW5/data/wine-evaluation-data.csv"

train <- read_csv(url)
eval <- read_csv(eval_url)

train <- train %>% dplyr::select(-INDEX)
```
## Data Summary

A table below expands on the variables included in analysis with comments from domain experts on expected effects. 


```{r}
table_def <- "
| **VARIABLE**         | **DEFINITION**                        | **THEORETICAL EFFECT**                             |
|:---------------------|:--------------------------------------|:---------------------------------------------------|
| `INDEX`              | ID Variable                           | None                                               |
| `TARGET`             | Cases Purchased                       | None                                               |
| `AcidIndex`          | Total Acidity Test                    | Unknown                                            |
| `Alcohol`            | Alcohol Content                       | Higher alcohol, higher sales                       |
| `Chlorides`          | Chloride Content                      | Low levels, higher quality                         |
| `CitricAcid`         | Citric Acid Content                   | Suggests freshness, impacts sales                  |
| `Density`            | Wine Density                          | Higher suggests richer wines                       |
| `FixedAcidity`       | Fixed Acidity                         | Affects taste                                      |
| `FreeSulfurDioxide`  | Free SO2 Content                      | Preserves freshness, impacts sales                 |
| `LabelAppeal`        | Label Appeal                          | More appealing, enhances sales                     |
| `ResidualSugar`      | Sugar Content                         | Sweetness impacts sales                            |
| `STARS`              | Expert Rating                         | Higher ratings, higher sales                       |
| `Sulphates`          | Sulfate Content                       | Affects preservation and taste                     |
| `TotalSulfurDioxide` | Total SO2                             | Affects longevity and freshness                    |
| `VolatileAcidity`    | Volatile Acid                         | Lower suggests higher quality                      |
| `pH`                 | pH                                    | Optimal pH impacts taste and stability             |
"
cat(table_def)
```

\newline
A quick look at the variables 5 number summary reveals that several variables have large ranges which when relating to their mean may suggest significantly different scales between variables, a high amount of skew, bi-modal distributions, or outliers. FixedAcidity, ResidualSugar, FreeSulfurDioxide, and TotalSulfurDioxide have fairly extreme ranges in comparison to their means.  Variables with Kurtosis greater than 4 will have observations distributed into heavy or long tails and may suggest numerous outliers, less than 2 suggest distributions centered around their mean with short or thin tails. Many of the variables are just below 2 suggesting many will have sharp peaks around the mean. Only AcidIndex shows as a non-ordinal or discrete distribution with extreme values of kurtosis, suggesting it will contain many outliers. 


```{r}
desc_train <- train %>% describe(omit = TRUE) %>% dplyr::select(-vars, -n)

desc_table <- desc_train |> 
  rownames_to_column('vars') %>% 
  mutate(
    range = cell_spec(range, color = ifelse(range > 50, "white", "black"),
                      background = ifelse(range > 50, "red", "white"),
                      bold = ifelse(range > 50, TRUE, FALSE)),
    skew = cell_spec(skew, color = ifelse(skew > 1 | skew < -1, "white", "black"),
                     background = ifelse(skew > 1 | skew < -1, "red", "white"),
                     bold = ifelse(skew > 1 | skew < -1, TRUE, FALSE)),
    kurtosis = cell_spec(kurtosis, color = ifelse(kurtosis > 4 | kurtosis < 1, "white", "black"),
                         background = ifelse(kurtosis > 4 | kurtosis < 1, "red", "white"),
                         bold = ifelse(kurtosis > 4 | kurtosis < 1, TRUE, FALSE))
  ) %>%
  column_to_rownames('vars') 

desc_table |> 
  kable(caption = "5 Number Summary", format = "latex", escape = FALSE, booktabs = TRUE, align = "l") %>%
  kable_styling(latex_options = "scale_down", full_width = FALSE)

```

\newpage
**Histograms**

As kurtosis foreshadowed, many of the distributions have sharp peaks at the mean with only the AcidIndex showing a bi-modal distribution. With the sharp centers around the peaks in the histograms, a high number of outliers may present themselves.
```{r histograms, echo=FALSE, warning=FALSE, fig.align='center'}
train %>%
  dplyr::select(-c(LabelAppeal, STARS, TARGET)) %>%
  gather(key = "variable", value = "value") %>%
  ggplot(aes(x = value)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = '#4E79A7', color = 'black') + 
  stat_density(geom = "line", color = "red") +
  facet_wrap(~ variable, scales = 'free') +
  theme(strip.text = element_text(size = 5), 
        panel.spacing = unit(0.5, "lines"),  
        strip.background = element_rect(fill = "lightblue", colour = "black", size = 0.5), 
        axis.text = element_text(size = 8),  
        axis.title = element_text(size = 8)) + 
  theme_bw()
```

\newpage
**Bar Plots**
There is a relatively normal distribution to LabelAppeal, but both STARS and TARGET tend to favor their lower values suggesting it's quite difficult to gain either a critic's praise or a significant amount of cases sold.
```{r barplots, fig.align='center'}

train %>% dplyr::select(c(LabelAppeal,STARS,TARGET)) |>
  gather(key = "variable", value = "value") |>  
  ggplot(aes(x = value)) + 
  geom_bar(aes(y = after_stat(count)), bins = 20, fill = '#4E79A7', color = 'black') +
  facet_wrap(~ variable, scales = 'free') +
  theme(strip.text = element_text(size = 5)) +
  theme_bw() +
  labs(y = "") 
```

**Box Plots**

Boxplots reveal a significant number of residuals in all of the variables.
\newline

```{r boxplot, fig.align='center'}
train %>%
  dplyr::select(-c(LabelAppeal, STARS, TARGET)) %>%
  gather(key = "Variable", value = "Value") %>%
  ggplot(aes(x = "", y = Value)) +  
  geom_boxplot(fill = "#4E79A7") +
  facet_wrap(~ Variable, scales = "free") + 
  labs(x = NULL, y = "Value") +  
  theme(strip.text = element_text(size = 5), 
        axis.text.x = element_blank(),  
        axis.ticks.x = element_blank()) 
```

**Correlation Matrix**

The correlation matrix reveals a moderate relationship between STARS and LabelAppeal with Target. Although both STARS and LabelAppeal seem to be somewhat correlated to each other suggesting potential colinearity. The AcidIndex, being a propietary method that aggregates across Acid metrics, does show some relationship with FixedAcidity but is relatively minor.

```{r corr-plot}
q <- cor(train, use = 'complete.obs')

ggcorrplot(q, type = "upper", outline.color = "white",
           ggtheme = theme_classic,
           colors = c("#F28E2B", "white", "#4E79A7"),
           lab = TRUE, show.legend = F, tl.cex = 5, lab_size = 3) 
```

**Missing Values**

While missing values may be indicative of the target, the STARS variable is missing 26% of its values. Determining the relationship it has to cases sold may be useful before removing it from the dataset. To view the relationship of the "missing" STARS ratings, NA's have been replaced with a category of "Unrated" and the bar plots are shown again. Chlorides, FreeSulfurDioxide, Alcohol, and TotalSulfurDioxide are missing around 5% or about 600 values. Sulphates is missing about 10% of its values and about 1200 values.
```{r percent missing check}
percentMiss <- function(x){sum(is.na(x))/length(x)*100} # Creates percentage of missing values

# Cut offs for variable dropping was 25% of values missing - none were dropped
# Cut offs for sample dropping was 50% of values missing - none were dropped

variable_pMiss <- apply(train,2,percentMiss) # 2 = runs on columns
sample_pMiss <- apply(train,1,percentMiss) # 1 = runs on rows

#sum(sample_pMiss > 50) 

pMiss <- data.frame(variables = names(variable_pMiss),pMiss = (variable_pMiss), row.names = NULL)
pMiss <- pMiss %>% arrange(desc(pMiss))


pMiss |>
  ggplot(aes(x = reorder(variables,pMiss), y = pMiss)) + 
  geom_bar(stat = 'identity', fill = '#4E79A7', color = 'black') +
  theme(strip.text = element_text(size = 5)) +
  theme_bw() +
  scale_x_discrete(guide = guide_axis(angle = 45))+
  labs(x = 'Variables',y = 'Percent Missing',title = 'Percent of Missing Values by Variable')

```


```{r missing-values, echo=FALSE}
missing_val <-
  train %>%
  summarise(across(everything(), ~ sum(is.na(.x)))) %>%
  select_if(function(.) last(.) != 0)

kbl(missing_val, caption = "Missing Values Count") |>
    kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")
```

```{r missing pattern, echo=TRUE}
plot_pattern(train, square = TRUE, rotate = TRUE, npat = 6)
```

While unrated wines typically aren't purchased, there are some that sell about 3 cases. This might suggest that non-rated wines are not submitted for critic's appraisal and should be used as a feature in the modeling. This plot also reveals a heavy preference for 2 star wines.

```{r}
train$STARS[is.na(train$STARS)] <- "Unrated"
train$STARS <- as.factor(train$STARS)
train$TARGET <- as.factor(train$TARGET)
train$LabelAppeal <- as.factor(train$LabelAppeal)

train %>% dplyr::select(c(LabelAppeal,STARS,TARGET)) |>
  gather(key = "variable", value = "value") |>  
  ggplot(aes(x = value)) + 
  geom_bar(aes(y = after_stat(count)), bins = 20, fill = '#4E79A7', color = 'black') +
  facet_wrap(~ variable, scales = 'free') +
  theme(strip.text = element_text(size = 5)) +
  theme_bw() +
  labs(y = "") 
```


```{r}
library(ggmosaic)
library(viridis)

ggplot(train, aes(STARS, TARGET)) + 
  scale_fill_viridis() +
  geom_bin2d()+
  labs(title = "Cases Sold vs STARS",
       subtitle = "STARS: 4 = Excellent; 1 = Poor")
```


The majority of 0 value appeals center on 4 cases sold and does tend to show a linear relationship between the two.
\newline

```{r}
ggplot(train, aes(LabelAppeal, TARGET)) + 
  scale_fill_viridis() +
  geom_bin2d()+
  labs(title = "Cases Sold vs LabelAppeal",
       subtitle = "LabelAppeal: 2 = High Appeal; -2 = Low Appeal")
```


# Data Preparation

Now that we have explored our data set, we can move on to data preparation to prep out data for modeling and analysis. 

## Data Wrangling

To do list:
1. Split the data into training and testing sets
2. Impute missing values
3. Normalize the data
4. Deal with outliers 
5. One hot encode the categorical variables



The data has already been partially cleaned with the removal of the INDEX variable. The missing values in STARS were replaced with "Unrated" to indicate non-rated wines.


### Data Imputation

Before we can impute missing values, we perform the train-test split to avoid data leakage: 

```{r}
set.seed(1125)

trainIndex <- createDataPartition(y = train$TARGET, p = 0.7, list = FALSE, times = 1)

train_data <- train[trainIndex,]
test_data <- train[-trainIndex,]
```

Now, we can impute the missing values in the training and testing data sets. We will use the MICE package to impute the missing values. In the imputation process, we will exclude the TARGET variable from the predictors, as the target variable should not be used to predict the missing values of the predictors. All imputation will be done for all three of the training, testing, and evaluation data sets. In order to make the dataframes match we drop the INDEX column from the evaluation data set.
```{r}

eval <- eval %>% dplyr::select(-IN)

train_data_no_target <- train_data[, !colnames(train_data) %in% c("TARGET")]
test_data_no_target <- test_data[, !colnames(test_data) %in% c("TARGET")]
eval_data_no_target <- eval[, !colnames(eval) %in% c("TARGET")]

combined_data <- rbind(train_data_no_target, test_data_no_target, eval_data_no_target)

data_type <- c(rep("train", nrow(train_data)), 
               rep("test", nrow(test_data)),
               rep("eval", nrow(eval)))

impute_func <- function(data, data_type) {
    ini <- mice(data, maxit = 0, ignore = data_type != "train")
    meth <- ini$meth
    imputed_object <- mice(data, method = meth, m = 5, maxit = 30, seed = 500, print = FALSE)
    imputed_data <- complete(imputed_object, "long")
    
    return(list(imputed_object = imputed_object, imputed_data = imputed_data))
}

results <- impute_func(combined_data, data_type)

reintegrate_targets <- function(imputed_data, original_data, target_vars) {
    if (!all(target_vars %in% colnames(original_data))) {
        stop("Target variables not found in the original data")
    }
    target_data <- original_data[target_vars]
    imputed_data_with_targets <- cbind(imputed_data, target_data)
    return(imputed_data_with_targets)
}

full_combined_data <- rbind(train_data, test_data, eval)

imputed_data_with_targets <- reintegrate_targets(results$imputed_data, full_combined_data, c("TARGET"))

train_data_imputed <- imputed_data_with_targets[data_type == "train", ]
test_data_imputed <- imputed_data_with_targets[data_type == "test", ]
eval_data_imputed <- imputed_data_with_targets[data_type == "eval", ]

train_data_imputed <- train_data_imputed[, !colnames(train_data_imputed) %in% c(".imp", ".id")]
test_data_imputed <- test_data_imputed[, !colnames(test_data_imputed) %in% c(".imp", ".id")]
tracking_df <- data.frame(eval_data_imputed)
eval_data_imputed <- eval_data_imputed[, !colnames(eval_data_imputed) %in% c(".imp", ".id")] 
```

Now that we've imputed the missing values, we can compare the summary statistics of the original data and the imputed data. The summary statistics are calculated for the following variables: Chlorides, FreeSulfurDioxide, Alcohol, TotalSulfurDioxide, pH, and Sulphates. The summary statistics are calculated for the full training data set, the training data set after imputation, and the testing data set after imputation. The summary statistics are calculated for the minimum, 1st quartile, median, mean, 3rd quartile, and maximum values of the variables. The summary statistics are then compared across the three data sets to see how the imputation process has affected the data.

```{r summary stats post imputations}
generate_summary <- function(data, vars, dataset_name) {
    summary_stats <- data %>%
        dplyr::select(all_of(vars)) %>%
        summarise(across(everything(), list(
            min = ~min(., na.rm = TRUE),
            q1 = ~quantile(., probs = 0.25, na.rm = TRUE),
            median = ~median(., na.rm = TRUE),
            mean = ~mean(., na.rm = TRUE),
            q3 = ~quantile(., probs = 0.75, na.rm = TRUE),
            max = ~max(., na.rm = TRUE)
        ))) %>%
        pivot_longer(cols = everything(), names_to = "Variable_Stat", values_to = "Value") %>%
        mutate(Dataset = dataset_name)
    return(summary_stats)
}

variables <- c("Chlorides", "FreeSulfurDioxide", "Alcohol", "TotalSulfurDioxide", "pH", "Sulphates")


summary_full_train <- generate_summary(train_data, variables, "Dataset (Pre-Imputations)")
summary_train_imputed <- generate_summary(train_data_imputed, variables, "Train Imputed")
summary_test_imputed <- generate_summary(test_data_imputed, variables, "Test Imputed")

combined_summary <- bind_rows(summary_full_train, summary_train_imputed, summary_test_imputed)

# pivoting wide so it's easier to compare
final_summary <- combined_summary %>%
    pivot_wider(names_from = Dataset, values_from = Value) %>% 
    mutate(across(where(is.numeric), ~format(., scientific = FALSE)))

kbl(final_summary, caption = "Summary Statistics Comparison Across Datasets") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")
```

The above table is very encouraging. The summary statistics for the variables with missing data did not seem to change much after the imputation. Most of the discrepancies appear in the test data set, this plausibly due to the smaller sample size.

### Transformations

```{r bestNormalize usage, warning=FALSE, message=FALSE, class.source = "fold-show", echo = FALSE}

variables <- c("FixedAcidity", "VolatileAcidity", "CitricAcid", "ResidualSugar", "Chlorides", "FreeSulfurDioxide", "TotalSulfurDioxide", "Density", "pH", "Sulphates", "Alcohol", "AcidIndex")

apply_best_normalize <- function(data, variables) {
  results <- data.frame(Variable = character(), Transformation = character(), stringsAsFactors = FALSE)
  
  for (var in variables) {
    has_negatives <- any(data[[var]] < 0, na.rm = TRUE)
    BN_object <- bestNormalize(data[[var]], allow.negative = has_negatives)
    #print(list(BN_object))
    
    if (is.list(BN_object$chosen_transform)) {
      best_method <- attr(BN_object$chosen_transform, "class")[1] 
    } else {
      best_method <- "Check Structure"  #In case structure is unexpected
    }
    
    results <- rbind(results, data.frame(Variable = var, Transformation = best_method))
   # cat("Best transformation for", var, ":", best_method, "\n")
  }
  
  return(results)
}

BN_results_train <- apply_best_normalize(train_data_imputed, variables)

kbl(BN_results_train, caption = "Best Transformations") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")

```

Again, we must be quite careful to avoid data leakage, calculating the parameters for these transformations using only the training set, and then applying the transformations to our other sets using the same parameters. 

```{r apply transformations, warning=FALSE, message=FALSE, class.source = "fold-show", echo = FALSE}

calculate_transformations <- function(data) {
  list(
    FixedAcidity_bn = orderNorm(data$FixedAcidity),
    VolatileAcidity_bn = orderNorm(data$VolatileAcidity),
    CitricAcid_bn = orderNorm(data$CitricAcid),
    ResidualSugar_bn = orderNorm(data$ResidualSugar),
    Chlorides_bn = orderNorm(data$Chlorides),
    FreeSulfurDioxide_bn = orderNorm(data$FreeSulfurDioxide),
    TotalSulfurDioxide_bn = orderNorm(data$TotalSulfurDioxide),
    Density_bn = orderNorm(data$Density),
    pH_bn = orderNorm(data$pH),
    Sulphates_bn = orderNorm(data$Sulphates),
    Alcohol_bn = orderNorm(data$Alcohol),
    AcidIndex_bn = orderNorm(data$AcidIndex)
  )
}

apply_pre_calculated_transformations <- function(data, transforms) {
  data %>%
    mutate(
      FixedAcidity_transformed = predict(transforms$FixedAcidity_bn, newdata = FixedAcidity),
      VolatileAcidity_transformed = predict(transforms$VolatileAcidity_bn, newdata = VolatileAcidity),
      CitricAcid_transformed = predict(transforms$CitricAcid_bn, newdata = CitricAcid),
      ResidualSugar_transformed = predict(transforms$ResidualSugar_bn, newdata = ResidualSugar),
      Chlorides_transformed = predict(transforms$Chlorides_bn, newdata = Chlorides),
      FreeSulfurDioxide_transformed = predict(transforms$FreeSulfurDioxide_bn, newdata = FreeSulfurDioxide),
      TotalSulfurDioxide_transformed = predict(transforms$TotalSulfurDioxide_bn, newdata = TotalSulfurDioxide),
      Density_transformed = predict(transforms$Density_bn, newdata = Density),
      pH_transformed = predict(transforms$pH_bn, newdata = pH),
      Sulphates_transformed = predict(transforms$Sulphates_bn, newdata = Sulphates),
      Alcohol_transformed = predict(transforms$Alcohol_bn, newdata = Alcohol),
      AcidIndex_transformed = predict(transforms$AcidIndex_bn, newdata = AcidIndex)
    )
}

transform_params <- calculate_transformations(train_data_imputed)

train_data_transformed <- apply_pre_calculated_transformations(train_data_imputed, transform_params)
test_data_transformed <- apply_pre_calculated_transformations(test_data_imputed, transform_params)
eval_data_transformed <- apply_pre_calculated_transformations(eval_data_imputed, transform_params)

```



```{r skew before and after}


pre_trans_skew <- summarise(train_data_imputed, 
                            across(c(FixedAcidity, VolatileAcidity, CitricAcid, ResidualSugar, Chlorides, FreeSulfurDioxide, TotalSulfurDioxide, Density, pH, Sulphates, Alcohol, AcidIndex),
                                   skewness, na.rm = T) %>% 
                            pivot_longer(everything(), names_to = "Variable", values_to = "Pre-Transformation Skew"))

post_trans_skew <- summarise(train_data_transformed, 
                             across(c(FixedAcidity_transformed, VolatileAcidity_transformed, CitricAcid_transformed, ResidualSugar_transformed, Chlorides_transformed, FreeSulfurDioxide_transformed, TotalSulfurDioxide_transformed, Density_transformed, pH_transformed, Sulphates_transformed, Alcohol_transformed, AcidIndex_transformed), 
                                    skewness, 
                                    na.rm = TRUE)) %>% 
                             pivot_longer(everything(), names_to = "Variable", values_to = "Post-Transformation Skew")

post_trans_skew$Variable <- sub("_transformed", "", post_trans_skew$Variable)

skewness_comparison <- left_join(pre_trans_skew, post_trans_skew, by = "Variable")

kbl(skewness_comparison, caption = "Pre and Post Transformation Skewness Comparison", digits = 3) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")
```

The transformations almost completely got rid of any skew in our data. We can visualize this using by recreating the histograms with the transformed data.

```{r histograms_2, echo=FALSE, warning=FALSE}
train_data_transformed %>% dplyr::select(-c(LabelAppeal,STARS,TARGET))|>
  gather(key = "variable", value = "value") |>  
  ggplot(aes(x = value)) + 
  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = '#4E79A7', color = 'black') + 
  stat_density(geom = "line", color = "red") +
  facet_wrap(~ variable, scales = 'free') +
  theme(strip.text = element_text(size = 5)) +
  theme_bw()

```

While the first table seems to suggest that AcidIndex_transformed had its skewness lowered to a relatively insignificant amount, the histogram reveals that the variable still seemingly is bimodal. This may suggest that the transformation was not the best choice for this variable and grouping the data may be a better choice. However, upon further investigation, the appearance of bimodality may be due to the amount of bins selected for the histograms. More bins reveal a more normal distribution. We can test whether it is bimodal using a dip test.


```{r dip-test}
dip_statistic <- dip.test(train_data_transformed$AcidIndex)

dip_statistic
```

The extremely low p-value suggests that the AcidIndex variable is bimodal. We will group the data into two categories to deal with this issue.

```{r}
variable <- train_data_imputed$AcidIndex

# Perform K-means clustering
k <- 2  # Number of clusters
km_clusters <- kmeans(variable, centers = k)

# Get cluster centroids
centroids <- km_clusters$centers

# Determine the split point (e.g., midpoint between centroids)
split_point <- mean(centroids)

# Create categorical variables based on the split point
data <- data.frame(
  variable = variable,
  group = ifelse(variable <= split_point, "Group 1", "Group 2")
)
```

```{r}
ggplot(data, aes(x = variable, fill = group)) +
  
  # Add histogram layer
  geom_histogram(binwidth = 0.5, alpha = 0.5, position = "identity") +
  
  # Add density plot layer
  geom_density(alpha = 0.5) +
  
  # Customize plot aesthetics
  labs(title = "Histogram with Density Plot Overlay", x = "Variable", y = "Density") +
  scale_fill_manual(values = c("Group 1" = "#4E79A7", "Group 2" = "#F28E2B")) + 
  facet_wrap(~ group) +  
  theme_minimal()
```

The resulting groups are shown in the histogram above. The plot reveals that, while not evenly distributed, there really is only one group. The appearance of bimodality is likely due to the much larger amount of the non-median group. We will not group the data and move on to dealing with outliers.

Now that we've transformed our data, we can move on to dealing with outliers.

### Outliers


```{r include=F}
calc_outliers <- function(data, columns) {
  sapply(columns, function(column) {
    Q1 <- quantile(data[[column]], 0.25, na.rm = TRUE)
    Q3 <- quantile(data[[column]], 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    list(lower = Q1 - 1.5 * IQR, upper = Q3 + 1.5 * IQR)
  }, simplify = FALSE)
}

limits <- calc_outliers(train_data_transformed, variables)
limits
```

We will use the IQR method to detect outliers in the data. The IQR method is a robust method for detecting outliers that is not sensitive to the presence of extreme values. The IQR method defines an outlier as any value that is below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR. The lower and upper limits for each variable are calculated using the IQR method.

Using the IQR limits, there is a significant amount of outliers in the data. The transformation process did not impact the number of outliers in the data. Using a Box-Cox transformation might have been a better way to get rid of the outliers but it was not an option for many of the variables due to them containing negative and zero values.

```{r include=F}
# Function to calculate the percentage of outliers for each column
calculate_outlier_percentage <- function(data, limits) {
  sapply(names(limits), function(column) {
    lower_limit <- limits[[column]]$lower
    upper_limit <- limits[[column]]$upper
    outliers <- sum(data[[column]] < lower_limit | data[[column]] > upper_limit, na.rm = TRUE)
    total <- length(data[[column]])
    percentage <- outliers / total * 100
    return(percentage)
  })
}

# Calculate outlier percentages using pre-calculated limits and the original dataset
outlier_percentages <- calculate_outlier_percentage(train_data_transformed, limits)

# Calculate outlier percentages for the original dataset
original_outlier_percentages <- calculate_outlier_percentage(train_data, limits)

# Create a data frame with outlier percentages for both datasets
outlier_comparison <- data.frame(
  Variable = names(outlier_percentages),
  Original_Data = outlier_percentages,
  Transformed_Data = outlier_percentages  # Assuming transformed data has already been calculated
)

# Create a kable table
outlier_table <- kbl(outlier_comparison, align = "c") %>%
  kable_paper(full_width = FALSE) %>%
  column_spec(1, bold = TRUE)  # Bold the variable names

outlier_table
```

Ultimately, due to the large amount of outliers, removing them would result in a significant loss of data. We will keep the outliers in the data and move on to one-hot encoding the categorical variables.

### One-Hot Encoding

We have two factor columns in LabelAppeal and STARS. LabelAppeal can be converted to numeric as it is ordinal. While STARS is also ordinal, it also has an 'unrated' category. We will one-hot encode this column but also keep the original column for now.

```{r}
cat_cols <- c("STARS")
dummy_data <- dummyVars(~ ., data = train_data_transformed[cat_cols], levelsOnly = FALSE)

# Apply the transformation to the datasets
train_data_encoded <- predict(dummy_data, newdata = train_data_transformed[cat_cols])
test_data_encoded <- predict(dummy_data, newdata = test_data_transformed[cat_cols])
eval_data_encoded <- predict(dummy_data, newdata = eval_data_transformed[cat_cols])

# Bind the encoded columns to the original datasets
train_data_prepped <- cbind(train_data_transformed, train_data_encoded)
test_data_prepped <- cbind(test_data_transformed, test_data_encoded)
eval_data_prepped <- cbind(eval_data_transformed, eval_data_encoded)

```

```{r}
#write_csv(train_data_prepped,"data\\train_final.csv")
#write_csv(test_data_prepped,"data\\test_final.csv")
#write_csv(eval_data_prepped,"data\\eval_final.csv")
```

```{r Write Data}
# Write CSV files to the specified directory
write_csv(train_data_imputed, "C:/Users/shaya/OneDrive/Documents/repos/Data622/HW4/Temp/train_final.csv")
write_csv(test_data_imputed, "C:/Users/shaya/OneDrive/Documents/repos/Data622/HW4/Temp/test_final.csv")
```

After our data has been prepped, we can now move on to modeling. 

# Modeling

With the data exploration and preparation out of the way, we turn to build different types of regression models to predict the number of cases of wine ordered by distributors. Again, the response variable is the *count* of cases, and so it is appropriate to consider Poisson regression, negative binomial regression, and multiple linear regression. We build models of each type with some commentary, and then we will consider more generally how the models compare to one another.

## Poisson Regression 

We first consider Poisson regression models. Now, it's critical to note, despite the transformations we performed in the previous section, Poisson models do not require normally distributed data, and so leveraging transformed data is actually counter-productive. As such, the relevant dataframes are:

* train_data_imputed 
* test_data_imputed
* eval_data_imputed

### Poisson Model 1

We start with a rather simple model, with all the variables along with a few more sophisticated variables:

1. Alcohol:LabelAppeal in case these two variables have an especially strong combined effect
2. STARS:Alcohol since high quality wines with certain alcohol content might sell especially well
3. LabelAppeal:STARS in case people might be especially likely to buy visually appealing and highly rated wines
4. Alcohol^2 as intuitively alcohol content doesn't have a strictly linear relationship with the target variable

```{r}
str(train_data_imputed$TARGET)

```


```{r}
# target variable MUST be numeric
train_data_imputed$TARGET <- as.integer(as.character(train_data_imputed$TARGET))

model_p_simple_1 <- glm(TARGET ~ Alcohol + Alcohol^2 + LabelAppeal + STARS + 
                    AcidIndex + Chlorides + CitricAcid + Density + 
                    FixedAcidity + FreeSulfurDioxide + ResidualSugar + 
                    Sulphates + TotalSulfurDioxide + VolatileAcidity + pH +
                    Alcohol:LabelAppeal + STARS:Alcohol + LabelAppeal:STARS,
                    family = poisson(), data = train_data_imputed)

summary(model_p_simple_1)
```

The model seems promising; for example there's major reduction in deviance from the null model to the full model. Still, there's much work to be done. We start by noting there are undefined coefficients because of singularities. Let's take a look at potential multicollinearity.

```{r}
# because the vif function wouldn't work:
alias(model_p_simple_1)

# and we rerun without the problematic interaction
model_p_simple_2 <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + FixedAcidity + 
                        FreeSulfurDioxide + ResidualSugar + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH + Alcohol:LabelAppeal + STARS:Alcohol,
                        family = poisson(), data = train_data_imputed)

summary(model_p_simple_2)

kbl(check_collinearity(model_p_simple_2), caption = "VIF Values simple model 2") %>%
  kable_classic(full_width = TRUE, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")

```


There's clearly a high degree of collinearity, but it's critical to remove one column at a time and reassess colinearity:

```{r}
#removing Alcohol:LabelAppeal
model_p_simple_3 <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + FixedAcidity + 
                        FreeSulfurDioxide + ResidualSugar + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH + STARS:Alcohol,
                        family = poisson(), data = train_data_imputed)

check_collinearity(model_p_simple_3)


```


```{r}
# removing Alcohol:STARS
model_p_simple_4 <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + FixedAcidity + 
                        FreeSulfurDioxide + ResidualSugar + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH,
                        family = poisson(), data = train_data_imputed)

kbl(check_collinearity(model_p_simple_4), caption = "VIF Values simple model 2") %>%
  kable_classic(full_width = TRUE, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")


```

At this point, we've removed all high correlation variables. Now, there are still two variables with fairly high VIFs, namely Alcohol and I(Alcohol^2)--this is unsurprising to say the least. It would be odd to remove only the former term, so let's see the model summary and consider whether we ought to remove I(Alcohol^2):

```{r}
summary(model_p_simple_4)
```

Indeed, I(Alcohol^2) *is* statistically significant, so we won't remove it. However, there are a couple variables that appear less promising, and we will removes those one at a time (a manual backwards elimination process).

```{r}
model_p_simple_5 <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density +
                        FreeSulfurDioxide + ResidualSugar + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH,
                        family = poisson(), data = train_data_imputed)

model_p_simple_6 <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + Density +
                        FreeSulfurDioxide + ResidualSugar + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH,
                        family = poisson(), data = train_data_imputed)

model_p_simple <- glm(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + Density +
                        FreeSulfurDioxide + Sulphates + TotalSulfurDioxide + 
                        VolatileAcidity + pH,
                        family = poisson(), data = train_data_imputed)

print(summary(model_p_simple))
```

There are a number of takeaways from this model summary, most of which are totally expected. First, only the quadratic term for alcohol is significant; this suggests that after a certain point, small changes in alcohol content--past a certain point--can have a large impact on the target variable. Second, the higher the label appeal level, the higher the log counts of cases ordered. Third, higher star ratings are highly associated with higher values for the target variable; being unrated significantly decreases the log count of cases ordered--we return to this point momentarily. Finally, a number of the chemical properties have effects on the target variable. For example, AcidIndex, Density, and VolatileAcidity all have negative coefficients. While I'm not a wine connoisseur myself, a highly dense wine seems unappealing  at least. 

Again, we note the huge reduction in deviance when going from the null model to the full model--this speaks well to our model. Now, a further question is if a Poisson model is appropriate here. A key condition for Poisson is that the mean and variance of the response variable are equal. We check this now: 

```{r check equidispersion}
residuals <- residuals(model_p_simple, type = "pearson")
chi_squared <- sum(residuals^2)
df <- df.residual(model_p_simple)

print(chi_squared / df)
```

So there certainly isn't over-dispersion. The under-dispersion is somewhat surprising, but the value is close enough to 1, and certainly close enough for a baseline model. We turn now to construct a new model

### Poisson Model 2 (Zero-Inflated)

We observed in our last model that unrated wines perform especially badly. Recall, though, we actually turned those values to unrated; they were missing at first. What if, then, these values should actually be a "0" rating? If so, we might be able to use a model that both improves accuracy and interpretability. The first step, then, is to create a new column changing the unrated values to zeros.

```{r}
train_data_imputed$original_stars <- as.numeric(as.character(train_data_imputed$STARS))
train_data_imputed$original_stars[train_data_imputed$STARS == "Unrated"] <- 0

eval_data_imputed$original_stars <- as.numeric(as.character(eval_data_imputed$STARS))
eval_data_imputed$original_stars[eval_data_imputed$STARS == "Unrated"] <- 0

test_data_imputed$original_stars <- as.numeric(as.character(test_data_imputed$STARS))
test_data_imputed$original_stars[test_data_imputed$STARS == "Unrated"] <- 0

value_counts <- table(train_data_imputed$original_stars)

kbl(value_counts, caption = "STARS Value Counts") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")

```

Immediately we see that this change leads to a large number of zeroes. This is a strong indicator for considering a zero-inflated model, especially given that a different process may well have led to a zero rating than the process that led to the other ratings. 

We start with creating a zero-inflated model using the same variables as the most recent Poisson model as that provides a strong baseline: 

```{r}
library(pscl)

zip_model_1 <- zeroinfl(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                      Sulphates + TotalSulfurDioxide + VolatileAcidity + pH | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

summary(zip_model_1)
```

It's quite interesting how this one change changed the model fairly significantly. We will finish removing variables, again using a backward elimination process, and then add more commentary. 


```{r}
zip_model_2 <- zeroinfl(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                      TotalSulfurDioxide + VolatileAcidity + pH | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

zip_model_3 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                      TotalSulfurDioxide + VolatileAcidity + pH | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

zip_model_4 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                      TotalSulfurDioxide + VolatileAcidity | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

zip_model_5 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density +
                      TotalSulfurDioxide + VolatileAcidity | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

zip_model <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                      AcidIndex + Chlorides + Density +
                      VolatileAcidity | 
                      original_stars,
                      data = train_data_imputed, dist = "poisson")

print(summary(zip_model))
```

There are many observations to be made. The first is that we used the p-value to eliminate predictors that were not significant, and it is striking that we were able to eliminate five variables once we switched to a zero-inflated model. Second, the quadratic alcohol term was one of those terms that was no longer significant. We were also able to eliminate Sulphates, pH, and the SulfurDioxide variables. As for the variables that persisted, the effects are not all that different: Label Appeal and Stars have a positive effect, chemical properties have negative effects. The key difference is that Alcohol now has a positive effect, but that's intuitive now that the previously positively impacting quadratic term is now removed.

As for the Stars variable (here called original_stars), again higher star ratings are associated with a higher log count of cases ordered. It's also the case that wines with no star ratings are more likely to have zero cases ordered. 

Let's now generate predictions for the two Poisson models: 

```{r}
predictions_poisson_1 <- predict(model_p_simple, newdata=test_data_imputed, type="response")
predictions_zip <- predict(zip_model, newdata=test_data_imputed, type="response")

test_data_imputed$TARGET <- as.numeric(as.character(test_data_imputed$TARGET))

# MAE, RMSE for simple Poisson

mae_poisson_1 <- mean(abs(predictions_poisson_1 - test_data_imputed$TARGET))
rmse_poisson_1 <- sqrt(mean((predictions_poisson_1 - test_data_imputed$TARGET)^2))

# MAE and RMSE for the ZIP model
mae_zip <- mean(abs(predictions_zip - test_data_imputed$TARGET))
rmse_zip <- sqrt(mean((predictions_zip - test_data_imputed$TARGET)^2))

cat("MAE Poisson Model: ", mae_poisson_1, "\n")
cat("RMSE Poisson Model: ", rmse_poisson_1, "\n")
cat("MAE ZIP Model: ", mae_zip, "\n")
cat("RMSE ZIP Model: ", rmse_zip, "\n")

```

Again, we will compare all models once all models are built, although it's worth noting that both MAE and RMSE values are pretty close to 1, which might be acceptable. But before we get ahead of ourselves, let's build the next two models. 

## Negative Binomial 

There is reason to believe that switching to a negative binomial model will yield better results. Specifically, the  negative binomial is appropriate when we are working with count data that has over-dispersion (the variance is greater than the mean). Now, it is true that earlier we saw under-dispersion relative to what one Poisson model expects. However, the truth is that it is really worthwhile to more get a direct measure of the dispersion in the outcome variable, before even modelling: 

```{r checking for overdispersion, echo=TRUE}
observed_variance <- var(train_data_imputed$TARGET)
expected_mean <- mean(train_data_imputed$TARGET)
print(observed_variance / expected_mean)

```

We see that this dispersion statistic is greater than 1. This suggests that we ought to try a negative binomial model.

### Negative Binomial Model 1

Much like earlier, we will start with a relatively simple model, at first using all the variables as well as the interaction terms attempted earlier, and then engaging in variable selection.

As a reminder, those additional variables are: 

1. Alcohol:LabelAppeal 
2. STARS:Alcohol 
3. Alcohol^2

(We omit LabelAppeal:STARS for the reason discussed earlier)

While it is true that most or all of these additional variables were not significant in the previous two models, it does not follow that they'll be insignificant in the negative binomial models.

```{r}
model_nb_simple_1 <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + 
                        FixedAcidity + FreeSulfurDioxide + ResidualSugar + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH +
                        Alcohol:LabelAppeal + STARS:Alcohol,
                        data = train_data_imputed)

summary(model_nb_simple_1)
```

If the previous model was any indication, there's likely high collinearity. Let's check:


```{r}
kbl(check_collinearity(model_nb_simple_1), caption = "VIF Values for NB Model") %>%
  kable_classic(full_width = TRUE, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")

```

Yet again, there are some extraordinarily high VIF values, likely because of the interaction terms. Again, then, we consult the VIF, remodel, and repeat until there are no variables with high collinearity.

```{r confronting collinearity, echo=FALSE, include=FALSE}
#removing Alcohol:LabelAppeal
model_nb_simple_2 <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + 
                        FixedAcidity + FreeSulfurDioxide + ResidualSugar + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH +
                        STARS:Alcohol,
                        data = train_data_imputed)

check_collinearity(model_nb_simple_2)

#removing Alcohol:STARS
model_nb_simple_3 <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + 
                        FixedAcidity + FreeSulfurDioxide + ResidualSugar + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH,
                        data = train_data_imputed)

check_collinearity(model_nb_simple_3)
```

```{r}
summary(model_nb_simple_3)
```

There are still a few columns that don't have significant predictors, and so again we consult the p-values to remove them one at a time:

```{r}
#removing FixedAcidity
model_nb_simple_4 <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + CitricAcid + Density + 
                        FreeSulfurDioxide + ResidualSugar + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH,
                        data = train_data_imputed)

summary(model_nb_simple_4)

#removing CitricAcid
model_nb_simple_5 <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + Density + 
                        FreeSulfurDioxide + ResidualSugar + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH,
                        data = train_data_imputed)

summary(model_nb_simple_5)

#removing ResidualSugar
model_nb_simple <- glm.nb(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + STARS + 
                        AcidIndex + Chlorides + Density + 
                        FreeSulfurDioxide + 
                        Sulphates + TotalSulfurDioxide + VolatileAcidity + pH,
                        data = train_data_imputed)

summary(model_nb_simple)

```

And so we arrive at a negative binomial model with some very interesting results. In particular, the estimates and standard errors are nearly identical as with the simple Poisson model! In fact, even the AIC values are quite similar. 

```{r comparison table}
summary_poisson <- summary(model_p_simple)
summary_nb <- summary(model_nb_simple)

poisson_results <- as.data.frame(summary_poisson$coefficients)
names(poisson_results) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")

nb_results <- as.data.frame(summary_nb$coefficients)
names(nb_results) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")

comparison_results <- data.frame(
  Term = rownames(poisson_results),
  Poisson_Estimate = poisson_results$Estimate,
  NB_Estimate = nb_results$Estimate,
  Poisson_Std_Error = poisson_results$`Std. Error`,
  NB_Std_Error = nb_results$`Std. Error`
)

comparison_results[,-1] <- round(comparison_results[,-1], 4)  

kbl(comparison_results, caption = "Comparison of Simple Poisson and Negatiive Binomial Models") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")
```



This is striking at first, and it is *possible* that it's due to convergence issues with the negative binomial model. It also seems that the dispersion is quite close to Poisson Assumptions (i.e. mean approximately equal to the variance), which is supported by the very high value of theta. Again, model selection will occur after all the models are built, but it certainly seems that there is no reason to accept this model over the simple Poisson one. 

Before we give up entirely on a negative binomial model, though, let's try a zero-inflated model as we did earlier:

### Negative Binomial Model 2 (Zero-Inflated)

We mimic the approach from earlier, starting by creating a zero-inflated model using the same variables as the most recent Negative Binomial model as that provides a strong baseline: 

```{r}
zinb_model_1 <- zeroinfl(TARGET ~ Alcohol + I(Alcohol^2) + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                         Sulphates + TotalSulfurDioxide + VolatileAcidity + pH | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model_1)

```

Notice again, the extremely high theta value suggests that the variance is very close to that of a Poisson distribution. The similarites in estimates and errors are thus predictable. Still, we'll complete the backward elimination before doing a proper comparison with the  Poisson zero-inflated model. 

```{r, include=FALSE}
#removing I(Alcohol^2) 
zinb_model_2 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                         Sulphates + TotalSulfurDioxide + VolatileAcidity + pH | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model_2)

#removing Sulphates
zinb_model_3 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                         TotalSulfurDioxide + VolatileAcidity + pH | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model_3)

#removing pH
zinb_model_4 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                         TotalSulfurDioxide + VolatileAcidity | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model_4)

#removing TotalSulfurDioxide
zinb_model_5 <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + FreeSulfurDioxide +
                         VolatileAcidity | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model_5)

```


```{r}
#removing FreeSulfurDioxide
zinb_model <- zeroinfl(TARGET ~ Alcohol + LabelAppeal + original_stars +
                         AcidIndex + Chlorides + Density + VolatileAcidity | 
                         original_stars,
                         data = train_data_imputed, dist = "negbin")

summary(zinb_model)
```

Note, these are *again* the same coefficients as in the zero-inflated Poisson--this despite the fact that we only used the p-values to guide our variable selection at this latter phase.

```{r comparison of zero inflated models}
summary_zip <- summary(zip_model_1)
summary_zinb <- summary(zinb_model_1)

zip_results <- as.data.frame(summary_zip$coefficients$count)
names(zip_results) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")

zinb_results <- as.data.frame(summary_zinb$coefficients$count)
names(zinb_results) <- c("Estimate", "Std. Error", "z value", "Pr(>|z|)")

zip_results[nrow(zip_results) + 1, ] <- NA
rownames(zip_results)[nrow(zip_results)] <- "Log(theta)"

comparison_results_zip_zinb <- data.frame(
  Term = rownames(zinb_results),  # Assume ZINB has all necessary rows including Log(theta)
  ZIP_Estimate = zip_results$Estimate,
  ZINB_Estimate = zinb_results$Estimate,
  ZIP_Std_Error = zip_results$`Std. Error`,
  ZINB_Std_Error = zinb_results$`Std. Error`
)

comparison_results_zip_zinb[,-1] <- round(comparison_results_zip_zinb[,-1], 4)

kbl(comparison_results_zip_zinb, caption = "Comparison of Zero-Inflated Poisson and Negatiive Binomial Models") %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling(latex_options = "HOLD_position")

```


So again, we are looking at nearly identical statistics from the zero-inflated Poisson to the zero-inflated negative binomial. Also again, we should prefer the zero-inflated Poisson to the zero-inflated negative binomial, since the distrubtion appars to b eclose neough to a Poisson.

And what of the comparison beetwene the two ngative binomial models? Of course, it is going to look extremeely similar to the comparison btween the two Poisson models. Still, we add it below for sake of completeneess: 


```{r}
predictions_nb <- predict(model_nb_simple, newdata=test_data_imputed, type="response")
predictions_zinb <- predict(zinb_model, newdata=test_data_imputed, type="response")

# MAE, RMSE for Negative Binomial

mae_nb_1 <- mean(abs(predictions_nb - test_data_imputed$TARGET))
rmse_nb_1 <- sqrt(mean((predictions_nb - test_data_imputed$TARGET)^2))

# MAE and RMSE for the ZINB model
mae_zinb <- mean(abs(predictions_zinb - test_data_imputed$TARGET))
rmse_zinb <- sqrt(mean((predictions_zinb - test_data_imputed$TARGET)^2))

cat("MAE Negative Binomial Model: ", mae_nb_1, "\n")
cat("RMSE Negative Binomial Model: ", rmse_nb_1, "\n")
cat("MAE Zero-Inflated Negative Binomial Model: ", mae_zinb, "\n")
cat("RMSE Zero-Inflated Negative Binomial Model: ", rmse_zinb, "\n")

```

And indeed, the comparison is extremely similar as to earlier. 

### Multiple Linear Regression 

We will look at a more direct approach with multiple linear regression using our normalized, transformed variables.

Looking at each predictor variable, we see that each predictor besides `FixedAcidity_transformed` are statistically significant within a 95% confidence level We also see that our $Adj. R^2 = 0.5405$, is where our model accounts on average for 54% of the variation of the `TARGET` variable. 

```{r mlr-model}
lm_model <- lm(as.numeric(as.character(TARGET)) ~ AcidIndex + FixedAcidity_transformed + VolatileAcidity_transformed + 
                 CitricAcid_transformed + ResidualSugar_transformed + Chlorides_transformed + 
                 FreeSulfurDioxide_transformed + TotalSulfurDioxide_transformed + 
                 Density_transformed + pH_transformed + Sulphates_transformed + 
                 Alcohol_transformed + STARS.1 + STARS.2 + STARS.3 + STARS.4 + 
                 STARS.Unrated + LabelAppeal, data=train_data_prepped)

summary(lm_model)
```

Calculating the RMSE for our training multiple regression model, we obtain an $RMSE = 1.3057$

```{r mlr-model-rmse}
sqrt(mean(lm_model$residuals^2))
```

### Stepwise Regression

We will create a stepwise regression model, to perform forward and backward elimination on our multiple linear regression attempt.

The results show that we strictly removed the `STARS.Unrated` column which makes sense, as it does not provide additional information as the other STARS columns accounts for it. The $Adj. R^2 = 0.5405$ which stayed the same as before, and the RMSE stayed the same as well. 

```{r step-mlr}
step_model <- stepAIC(lm_model)
summary(step_model, direction="both")
```

```{r step-model-rmse}
sqrt(mean(step_model$residuals^2))
```


### Model Selection

Let's now look at all the models compared to each other based on their results with using the test data. The best model in comparison to the others is the original theory of using Poisson with the best $R^2=0.5611089$ and the lowest $RMSE=1.2757393$. We could go with the basis of the AIC metric, however, our goal is to predict the best results with our evaluation set. Even with all the transformations, interactions and trying to handle zero inflation factors, a Poisson model is our best choice to predict on the evaluation set. 

Again AIC would be a better choice if we wanted to have more of an inference between the models. 

```{r model-results}
test_data_prepped$TARGET <- as.numeric(as.character(test_data_prepped$TARGET))
predictions_step <- predict(step_model, newdata = test_data_prepped)
model_results <- data.frame(obs = test_data_prepped$TARGET, pred=predictions_step)
colnames(model_results) = c('obs', 'pred')
  
# caret has a way to provide summary performance stats 
stepwise_performance <- as.tibble(defaultSummary(model_results), rownames = " ") |>
  rename(Stepwise = value)

# Poisson Model
model_results <- data.frame(obs = test_data_imputed$TARGET, pred=predictions_poisson_1)
colnames(model_results) = c('obs', 'pred')
poisson_performance <- as.tibble(defaultSummary(model_results), rownames = " ") |>
  rename(Poisson = value)

# Zero_Inflated Poisson Model
model_results <- data.frame(obs = test_data_imputed$TARGET, pred=predictions_zip)
colnames(model_results) = c('obs', 'pred')
zip_performance <- as.tibble(defaultSummary(model_results), rownames = " ") |>
  rename(ZI_Poisson = value)

# Negative Binomial Model
model_results <- data.frame(obs = test_data_imputed$TARGET, pred=predictions_nb)
colnames(model_results) = c('obs', 'pred')
nb_performance <- as.tibble(defaultSummary(model_results), rownames = " ") |>
  rename(Neg_Binom = value)

# Zero-Inflated Negative Binomial Model
model_results <- data.frame(obs = test_data_imputed$TARGET, pred=predictions_zinb)
colnames(model_results) = c('obs', 'pred')
zinb_performance <- as.tibble(defaultSummary(model_results), rownames = " ") |>
  rename(ZI_Neg_Binom = value)


models_performance <- left_join(poisson_performance, zip_performance, by=" ") %>%
    left_join(., nb_performance, by=" ") %>%
    left_join(., zinb_performance, by=" ") %>%
    left_join(., stepwise_performance, by=" ") %>%
    add_row(., " " = "AIC", 
            Poisson = model_p_simple$aic, 
            ZI_Poisson = AIC(zip_model), 
            Neg_Binom = model_nb_simple$aic, 
            ZI_Neg_Binom = AIC(zinb_model), 
            Stepwise = AIC(step_model))

options(scipen=999)
  
kbl(models_performance, caption = "Model Performance") |>
  kable_classic(full_width = F, html_font = "Cambria") |>
  kable_styling(latex_options = "HOLD_position")
```

### Predictions

### Predictions

As we have now selected our models, we are ready to make predictions on the evaluation set. This is a slightly complicated process because our second model is dependent on our first one. We complete this process below:

```{r}
eval_predictions_poisson <- predict(model_p_simple, newdata=eval_data_imputed, type="response")

eval_predictions_only <- as.tibble(eval_predictions_poisson) |>
  rename(TARGET = value) |>
  mutate(TARGET = round(TARGET))

write.csv(eval_predictions_only, "Eval_Predictions.csv", row.names = FALSE)

set.seed(123)

kbl(head(eval_predictions_only, 10), caption = "Preview: Predictions for Evaluation Dataset") |>
  kable_classic(full_width = F, html_font = "Cambria") |>
  kable_styling(latex_options = "HOLD_position")

```

---

```{r}
# First, make sure you have the 'reticulate' package installed
#install.packages("reticulate")

# Load the reticulate library in R
library(reticulate)

# Set your Python version if necessary
use_python("C:/Users/shaya/AppData/Local/Programs/Python/Python312/python.exe", required = TRUE)

# You can also check which Python version is being used
py_config()

# Install necessary Python packages (if not already installed)
py_install(c("pandas", "numpy", "scikit-learn", "statsmodels", "matplotlib", "tensorflow", "keras"))
```


```{python}
# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.api import GLM, add_constant, NegativeBinomial
import matplotlib.pyplot as plt

# Load the dataset from local paths
train_data = pd.read_csv(r"C:\Users\shaya\OneDrive\Documents\repos\Data622\HW4\Temp\train_final.csv")
test_data = pd.read_csv(r"C:\Users\shaya\OneDrive\Documents\repos\Data622\HW4\Temp\test_final.csv")

# Replace "Unrated" in the STARS column with -1 and ensure the column is numeric
for dataset in [train_data, test_data]:
    dataset["STARS"] = dataset["STARS"].replace("Unrated", -1)
    dataset["STARS"] = pd.to_numeric(dataset["STARS"], errors="coerce")  # Coerce non-convertible values to NaN

# Specify the relevant columns for the model
relevant_columns = [
    "Alcohol", "LabelAppeal", "STARS", "AcidIndex", "Chlorides", "CitricAcid", 
    "Density", "FixedAcidity", "FreeSulfurDioxide", "ResidualSugar", 
    "Sulphates", "TotalSulfurDioxide", "VolatileAcidity", "pH"
]

# Create a subset of the data with only the relevant columns (excluding 'TARGET')
X_train = train_data[relevant_columns]
X_test = test_data[relevant_columns]

# Add squared Alcohol column to both train and test sets
X_train["Alcohol_squared"] = X_train["Alcohol"] ** 2
X_test["Alcohol_squared"] = X_test["Alcohol"] ** 2

# Create interaction terms as specified in the formula
X_train["Alcohol_LabelAppeal"] = X_train["Alcohol"] * X_train["LabelAppeal"]
X_test["Alcohol_LabelAppeal"] = X_test["Alcohol"] * X_test["LabelAppeal"]
X_train["STARS_Alcohol"] = X_train["STARS"] * X_train["Alcohol"]
X_test["STARS_Alcohol"] = X_test["STARS"] * X_test["Alcohol"]
X_train["LabelAppeal_STARS"] = X_train["LabelAppeal"] * X_train["STARS"]
X_test["LabelAppeal_STARS"] = X_test["LabelAppeal"] * X_test["STARS"]

# Add constant to features for intercept in GLM (Generalized Linear Model)
X_train_const = add_constant(X_train)
X_test_const = add_constant(X_test)

# Define the target variable
y_train = train_data["TARGET"]
y_test = test_data["TARGET"]

# Now you have X_train, X_test, y_train, and y_test ready for modeling


# At this point, X_train_const, X_test_const, y_train, y_test are ready for further analysis or modeling
# --- Model 1: Negative Binomial Regression (Recreated from R Script) ---
nb_model = GLM(y_train, X_train_const).fit()

# Predictions and evaluation
nb_predictions = nb_model.predict(X_test_const)
nb_mse = mean_squared_error(y_test, nb_predictions)
nb_mae = mean_absolute_error(y_test, nb_predictions)

print("Negative Binomial Regression Results:")
print(f"MSE: {nb_mse}")
print(f"MAE: {nb_mae}")

# --- Model 2: Neural Network (Imported from Python Notebook) ---
def normalize_data(df):
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Normalize data for Neural Network
X_train_nn = normalize_data(X_train)
X_test_nn = normalize_data(X_test)
y_train_nn = y_train.values.reshape(-1, 1)
y_test_nn = y_test.values.reshape(-1, 1)

# Neural Network Model
import tensorflow as tf
from tensorflow import keras

nn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train_nn.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])

nn_model.compile(
    loss='mse',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['mae']
)

nn_model.fit(
    X_train_nn, y_train_nn,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=0
)

# Predictions and evaluation
nn_predictions = nn_model.predict(X_test_nn)
nn_mse = mean_squared_error(y_test_nn, nn_predictions)
nn_mae = mean_absolute_error(y_test_nn, nn_predictions)

print("\nNeural Network Results:")
print(f"MSE: {nn_mse}")
print(f"MAE: {nn_mae}")

# --- Comparison and Visualization ---
metrics = pd.DataFrame({
    "Model": ["Negative Binomial Regression", "Neural Network"],
    "MSE": [nb_mse, nn_mse],
    "MAE": [nb_mae, nn_mae]
})

print("\nModel Comparison:")
print(metrics)

# Bar plot for comparison
metrics.set_index("Model").plot(kind="bar", figsize=(8, 6))
plt.title("Model Comparison: MSE and MAE")
plt.ylabel("Error")
plt.xticks(rotation=0)
plt.grid(axis="y")
plt.show()
```

```{r}
# Save the predicted values for the *test* data (not the evaluation data)
write.csv(model_results, "Model_Results.csv", row.names = FALSE)
```


# Conclusion: 

In this project, we aimed to develop predictive models for wine sales using statistical techniques and machine learning algorithms. We started by exploring the data, handling missing values, and transforming variables. Initially, we experimented with Poisson regression and zero-inflated Poisson models, as well as negative binomial regression to address over-dispersion. However, the simple Poisson model emerged as the best performer. We refined our models through stepwise regression but found no significant improvement over the simple Poisson model. Overall, our models offer valuable insights for wine producers and distributors, aiding in resource allocation and marketing strategy adjustments to optimize sales and enhance profitability in the wine industry.

---

```{r}
# First, make sure you have the 'reticulate' package installed
#install.packages("reticulate")

# Load the reticulate library in R
library(reticulate)

# Set your Python version if necessary
use_python("C:/Users/shaya/AppData/Local/Programs/Python/Python312/python.exe", required = TRUE)

# You can also check which Python version is being used
py_config()

# Install necessary Python packages (if not already installed)
py_install(c("pandas", "numpy", "scikit-learn", "statsmodels", "matplotlib", "tensorflow", "keras"))
```


```{python}
# Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from statsmodels.api import GLM, add_constant, NegativeBinomial
import matplotlib.pyplot as plt

# Load the dataset from local paths
train_data = pd.read_csv(r"C:\Users\shaya\OneDrive\Documents\repos\Data622\HW4\Temp\train_final.csv")
test_data = pd.read_csv(r"C:\Users\shaya\OneDrive\Documents\repos\Data622\HW4\Temp\test_final.csv")

# Replace "Unrated" in the STARS column with -1 and ensure the column is numeric
for dataset in [train_data, test_data]:
    dataset["STARS"] = dataset["STARS"].replace("Unrated", -1)
    dataset["STARS"] = pd.to_numeric(dataset["STARS"], errors="coerce")  # Coerce non-convertible values to NaN

# Specify the relevant columns for the model
relevant_columns = [
    "Alcohol", "LabelAppeal", "STARS", "AcidIndex", "Chlorides", "CitricAcid", 
    "Density", "FixedAcidity", "FreeSulfurDioxide", "ResidualSugar", 
    "Sulphates", "TotalSulfurDioxide", "VolatileAcidity", "pH"
]

# Create a subset of the data with only the relevant columns (excluding 'TARGET')
X_train = train_data[relevant_columns]
X_test = test_data[relevant_columns]

# Add squared Alcohol column to both train and test sets
X_train["Alcohol_squared"] = X_train["Alcohol"] ** 2
X_test["Alcohol_squared"] = X_test["Alcohol"] ** 2

# Create interaction terms as specified in the formula
X_train["Alcohol_LabelAppeal"] = X_train["Alcohol"] * X_train["LabelAppeal"]
X_test["Alcohol_LabelAppeal"] = X_test["Alcohol"] * X_test["LabelAppeal"]
X_train["STARS_Alcohol"] = X_train["STARS"] * X_train["Alcohol"]
X_test["STARS_Alcohol"] = X_test["STARS"] * X_test["Alcohol"]
X_train["LabelAppeal_STARS"] = X_train["LabelAppeal"] * X_train["STARS"]
X_test["LabelAppeal_STARS"] = X_test["LabelAppeal"] * X_test["STARS"]

# Add constant to features for intercept in GLM (Generalized Linear Model)
X_train_const = add_constant(X_train)
X_test_const = add_constant(X_test)

# Define the target variable
y_train = train_data["TARGET"]
y_test = test_data["TARGET"]

# Now you have X_train, X_test, y_train, and y_test ready for modeling


# At this point, X_train_const, X_test_const, y_train, y_test are ready for further analysis or modeling
# --- Model 1: Negative Binomial Regression (Recreated from R Script) ---
nb_model = GLM(y_train, X_train_const).fit()

# Predictions and evaluation
nb_predictions = nb_model.predict(X_test_const)
nb_mse = mean_squared_error(y_test, nb_predictions)
nb_mae = mean_absolute_error(y_test, nb_predictions)

print("Negative Binomial Regression Results:")
print(f"MSE: {nb_mse}")
print(f"MAE: {nb_mae}")

# --- Model 2: Neural Network (Imported from Python Notebook) ---
def normalize_data(df):
    from sklearn.preprocessing import MinMaxScaler
    scaler = MinMaxScaler()
    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Normalize data for Neural Network
X_train_nn = normalize_data(X_train)
X_test_nn = normalize_data(X_test)
y_train_nn = y_train.values.reshape(-1, 1)
y_test_nn = y_test.values.reshape(-1, 1)

# Neural Network Model
import tensorflow as tf
from tensorflow import keras

nn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train_nn.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)
])

nn_model.compile(
    loss='mse',
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=['mae']
)

nn_model.fit(
    X_train_nn, y_train_nn,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    verbose=0
)

# Predictions and evaluation
nn_predictions = nn_model.predict(X_test_nn)
nn_mse = mean_squared_error(y_test_nn, nn_predictions)
nn_mae = mean_absolute_error(y_test_nn, nn_predictions)

print("\nNeural Network Results:")
print(f"MSE: {nn_mse}")
print(f"MAE: {nn_mae}")

# --- Comparison and Visualization ---
metrics = pd.DataFrame({
    "Model": ["Negative Binomial Regression", "Neural Network"],
    "MSE": [nb_mse, nn_mse],
    "MAE": [nb_mae, nn_mae]
})

print("\nModel Comparison:")
print(metrics)

# Bar plot for comparison
metrics.set_index("Model").plot(kind="bar", figsize=(8, 6))
plt.title("Model Comparison: MSE and MAE")
plt.ylabel("Error")
plt.xticks(rotation=0)
plt.grid(axis="y")
plt.show()
```
